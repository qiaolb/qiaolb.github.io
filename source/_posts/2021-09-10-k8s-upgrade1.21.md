---
layout: blog
title: Kubernetes升级1.21问题汇总
date: 2021-09-10 08:00:00
tags:
    - k8s
    - cert
---


将`Kubernetes`从`1.20`升级到`1.21`时，遇到了两个问题。

### `Core DNS` `image`下载失败
通过以下命令进行`image`下载：

```
> kubeadm config images pull --image-repository registry.aliyuncs.com/google_containers     
I0910 08:00:20.155181   16619 version.go:254] remote version is much newer: v1.22.1; falling back to: stable-1.21
[config/images] Pulled registry.aliyuncs.com/google_containers/kube-apiserver:v1.21.4
[config/images] Pulled registry.aliyuncs.com/google_containers/kube-controller-manager:v1.21.4
[config/images] Pulled registry.aliyuncs.com/google_containers/kube-scheduler:v1.21.4
[config/images] Pulled registry.aliyuncs.com/google_containers/kube-proxy:v1.21.4
[config/images] Pulled registry.aliyuncs.com/google_containers/pause:3.4.1
[config/images] Pulled registry.aliyuncs.com/google_containers/etcd:3.4.13-0
failed to pull image "registry.aliyuncs.com/google_containers/coredns:v1.8.0": output: Error response from daemon: manifest for registry.aliyuncs.com/google_containers/coredns:v1.8.0 not found: manifest unknown: manifest unknown
, error: exit status 1
To see the stack trace of this error execute with --v=5 or higher
```

`registry.aliyuncs.com/google_containers/coredns:v1.8.0`一直下载不成功，经过检查发现，该镜像的`tag`为`1.8.0`不是`v1.8.0`，我们直接手工下载，重新设置tag即可：

对于`docker`运行时方法如下：

```
docker pull registry.aliyuncs.com/google_containers/coredns:1.8.0
docker tag registry.aliyuncs.com/google_containers/coredns:1.8.0 registry.aliyuncs.com/google_containers/coredns:v1.8.0
```

`containerd`方法如下：
```
crictl pull registry.aliyuncs.com/google_containers/coredns:1.8.0
ctr -n k8s.io i tag registry.aliyuncs.com/google_containers/coredns:1.8.0 registry.aliyuncs.com/google_containers/coredns:v1.8.0
```

然后在正常升级就可以了。

### `Failed to start ContainerManager`异常

升级完成后，发现工作节点上，`kubelet`启动异常，如下：
```
Failed to start ContainerManager failed to initialise top level QOS containers
```

解决办法：
```
systemctl stop kubepods-burstable.slice
systemctl restart kubelet
```

注意，这样会导致部署在该节点的所有`Pod`重启，重启后就正常了。
